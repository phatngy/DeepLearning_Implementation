{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NguyenVanPhat.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFrLmoNvyLMw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "a543425c-bc98-4c06-a325-bf7cad150241"
      },
      "source": [
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten, add, Dropout\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hx-8v4OsU0Y",
        "colab_type": "code",
        "outputId": "14b6b74f-d5f8-47e7-d649-108057fadec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HQKOgjxwbnV",
        "colab_type": "code",
        "outputId": "584d5575-045f-416c-d581-3ff9e68f14f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%cd '/content/gdrive/My Drive/DLCB'\n",
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/DLCB\n",
            "mnist_autoencoder.h5  NguyenVanPhat.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHZZYpmHyT03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "epochs = 200\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "num_filters = 16\n",
        "kernel_size = 3\n",
        "n = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6fi3KNCyajO",
        "colab_type": "code",
        "outputId": "47879867-e160-4241-d966-d199522f8d31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, Y_train), (x_test, y_test) = cifar10.load_data()\n",
        " \n",
        "x_train, y_train = X_train[:40000, :], Y_train[:40000]\n",
        "x_val, y_val = X_train[40000: 50000, :], Y_train[40000: 50000,]\n",
        "x_train, x_val, x_test = x_train.astype('float32')/255, x_val.astype('float32')/255, x_test.astype('float32')/255\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_val = to_categorical(y_val, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "input_shape = x_train.shape[1:]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEcRYe31VhR7",
        "colab_type": "code",
        "outputId": "0173791c-605b-4c32-f5e8-4240f3cacad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(y_train[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6cWJrgvyfSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resnet_block(x, num_filter=num_filters, increase=False):\n",
        "    stride = 1\n",
        "\n",
        "    if increase:\n",
        "        stride = 2\n",
        "\n",
        "    y1 = Activation('relu')(BatchNormalization()(x))\n",
        "    conv_1 = Conv2D(filters=num_filter, kernel_size=3, strides=stride, padding='same', kernel_initializer=\"he_normal\")(y1)\n",
        "    y2 = Activation('relu')(BatchNormalization()(conv_1))\n",
        "    conv_2 = Conv2D(filters=num_filter, kernel_size=3, strides=1, padding='same', kernel_initializer=\"he_normal\")(y2)\n",
        "\n",
        "    if increase:\n",
        "        y = Conv2D(filters=num_filter, kernel_size=1, strides=stride, padding='same', kernel_initializer=\"he_normal\")(y1)\n",
        "        block = add([conv_2, y])\n",
        "    else:\n",
        "        block = add([conv_2, x])\n",
        "    return block"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn27s_rE38bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = Input(shape=input_shape)\n",
        "x = Conv2D(filters=num_filters, kernel_size=3, strides=1, padding='same', kernel_initializer=\"he_normal\")(inputs)\n",
        "\n",
        "for _ in range(n):\n",
        "    x = resnet_block(x, 16, False)\n",
        "\n",
        "\n",
        "x = resnet_block(x, 32, True)\n",
        "for _ in range(1, n):\n",
        "    x = resnet_block(x, 32, False)\n",
        "\n",
        "\n",
        "x = resnet_block(x, 64, True)\n",
        "for _ in range(1, n):\n",
        "    x = resnet_block(x, 64, False)\n",
        "    \n",
        "x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "x = AveragePooling2D(pool_size=8)(x)\n",
        "x = Flatten()(x)\n",
        "\n",
        "outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTm0aVYba2t6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdX4hJ_v6iG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uNJVAKTArXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvKRjjFr67nT",
        "colab_type": "code",
        "outputId": "c60dc381-b773-4928-b9b5-a6aaeae9c6d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('Using real-time data augmentation.')\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
        "\n",
        "callbacks = [lr_reducer, lr_scheduler]\n",
        "\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size), validation_data=(x_val, y_val), epochs=epochs, verbose=True,\n",
        "                  workers=4, shuffle=True, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n",
            "Epoch 1/200\n",
            "313/313 [==============================] - 63s 201ms/step - loss: 1.5686 - acc: 0.4188 - val_loss: 1.6242 - val_acc: 0.4291\n",
            "Epoch 2/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 1.2204 - acc: 0.5618 - val_loss: 1.9158 - val_acc: 0.4385\n",
            "Epoch 3/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 1.0632 - acc: 0.6224 - val_loss: 1.2914 - val_acc: 0.5732\n",
            "Epoch 4/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.9378 - acc: 0.6705 - val_loss: 1.2224 - val_acc: 0.5725\n",
            "Epoch 5/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.8502 - acc: 0.6999 - val_loss: 1.0769 - val_acc: 0.6317\n",
            "Epoch 6/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.7875 - acc: 0.7231 - val_loss: 0.9854 - val_acc: 0.6744\n",
            "Epoch 7/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.7251 - acc: 0.7479 - val_loss: 0.9266 - val_acc: 0.6952\n",
            "Epoch 8/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.6827 - acc: 0.7620 - val_loss: 0.8728 - val_acc: 0.7134\n",
            "Epoch 9/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.6470 - acc: 0.7746 - val_loss: 0.9376 - val_acc: 0.6950\n",
            "Epoch 10/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.6173 - acc: 0.7866 - val_loss: 0.8778 - val_acc: 0.7076\n",
            "Epoch 11/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.5913 - acc: 0.7943 - val_loss: 0.6633 - val_acc: 0.7796\n",
            "Epoch 12/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.5579 - acc: 0.8047 - val_loss: 0.7682 - val_acc: 0.7474\n",
            "Epoch 13/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.5417 - acc: 0.8114 - val_loss: 0.7584 - val_acc: 0.7468\n",
            "Epoch 14/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.5216 - acc: 0.8174 - val_loss: 0.7483 - val_acc: 0.7477\n",
            "Epoch 15/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.4998 - acc: 0.8277 - val_loss: 0.6342 - val_acc: 0.7856\n",
            "Epoch 16/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.4814 - acc: 0.8335 - val_loss: 0.6426 - val_acc: 0.7896\n",
            "Epoch 17/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.4716 - acc: 0.8360 - val_loss: 0.5535 - val_acc: 0.8149\n",
            "Epoch 18/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.4522 - acc: 0.8436 - val_loss: 0.7938 - val_acc: 0.7536\n",
            "Epoch 19/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.4417 - acc: 0.8477 - val_loss: 0.5939 - val_acc: 0.8036\n",
            "Epoch 20/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.4330 - acc: 0.8470 - val_loss: 0.7102 - val_acc: 0.7763\n",
            "Epoch 21/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.4167 - acc: 0.8564 - val_loss: 0.5792 - val_acc: 0.8124\n",
            "Epoch 22/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.4059 - acc: 0.8584 - val_loss: 0.6543 - val_acc: 0.7960\n",
            "Epoch 23/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.3923 - acc: 0.8639 - val_loss: 0.5065 - val_acc: 0.8336\n",
            "Epoch 24/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.3848 - acc: 0.8655 - val_loss: 0.5159 - val_acc: 0.8291\n",
            "Epoch 25/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.3774 - acc: 0.8668 - val_loss: 0.4902 - val_acc: 0.8389\n",
            "Epoch 26/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.3654 - acc: 0.8734 - val_loss: 0.7019 - val_acc: 0.7916\n",
            "Epoch 27/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.3583 - acc: 0.8750 - val_loss: 0.6148 - val_acc: 0.8101\n",
            "Epoch 28/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.3485 - acc: 0.8777 - val_loss: 0.5635 - val_acc: 0.8220\n",
            "Epoch 29/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.3392 - acc: 0.8822 - val_loss: 0.5196 - val_acc: 0.8318\n",
            "Epoch 30/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.3330 - acc: 0.8829 - val_loss: 0.6054 - val_acc: 0.8156\n",
            "Epoch 31/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.3209 - acc: 0.8867 - val_loss: 0.4613 - val_acc: 0.8500\n",
            "Epoch 32/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.3218 - acc: 0.8877 - val_loss: 0.6686 - val_acc: 0.7992\n",
            "Epoch 33/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.3141 - acc: 0.8893 - val_loss: 0.5811 - val_acc: 0.8241\n",
            "Epoch 34/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.3094 - acc: 0.8903 - val_loss: 0.6505 - val_acc: 0.8114\n",
            "Epoch 35/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2948 - acc: 0.8973 - val_loss: 0.6327 - val_acc: 0.8017\n",
            "Epoch 36/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2907 - acc: 0.8983 - val_loss: 0.6755 - val_acc: 0.8058\n",
            "Epoch 37/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2904 - acc: 0.8989 - val_loss: 0.5835 - val_acc: 0.8269\n",
            "Epoch 38/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.2874 - acc: 0.8991 - val_loss: 0.5184 - val_acc: 0.8426\n",
            "Epoch 39/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.2742 - acc: 0.9025 - val_loss: 0.9097 - val_acc: 0.7614\n",
            "Epoch 40/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.2754 - acc: 0.9033 - val_loss: 0.4523 - val_acc: 0.8557\n",
            "Epoch 41/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.2636 - acc: 0.9068 - val_loss: 0.7596 - val_acc: 0.7860\n",
            "Epoch 42/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2653 - acc: 0.9058 - val_loss: 0.4893 - val_acc: 0.8490\n",
            "Epoch 43/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.2559 - acc: 0.9087 - val_loss: 0.5385 - val_acc: 0.8346\n",
            "Epoch 44/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2561 - acc: 0.9097 - val_loss: 0.4791 - val_acc: 0.8533\n",
            "Epoch 45/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2453 - acc: 0.9140 - val_loss: 0.5833 - val_acc: 0.8261\n",
            "Epoch 46/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2379 - acc: 0.9157 - val_loss: 0.4485 - val_acc: 0.8637\n",
            "Epoch 47/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.2388 - acc: 0.9161 - val_loss: 0.4252 - val_acc: 0.8663\n",
            "Epoch 48/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2367 - acc: 0.9173 - val_loss: 0.5327 - val_acc: 0.8419\n",
            "Epoch 49/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2326 - acc: 0.9166 - val_loss: 0.4639 - val_acc: 0.8613\n",
            "Epoch 50/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.2311 - acc: 0.9198 - val_loss: 0.4940 - val_acc: 0.8479\n",
            "Epoch 51/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2268 - acc: 0.9199 - val_loss: 0.4551 - val_acc: 0.8643\n",
            "Epoch 52/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.2190 - acc: 0.9223 - val_loss: 0.4665 - val_acc: 0.8597\n",
            "Epoch 53/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2135 - acc: 0.9235 - val_loss: 0.5105 - val_acc: 0.8565\n",
            "Epoch 54/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.2161 - acc: 0.9242 - val_loss: 0.4621 - val_acc: 0.8567\n",
            "Epoch 55/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2092 - acc: 0.9263 - val_loss: 0.4925 - val_acc: 0.8566\n",
            "Epoch 56/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.2112 - acc: 0.9250 - val_loss: 0.6452 - val_acc: 0.8236\n",
            "Epoch 57/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.2023 - acc: 0.9278 - val_loss: 0.5327 - val_acc: 0.8465\n",
            "Epoch 58/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.2026 - acc: 0.9281 - val_loss: 0.5370 - val_acc: 0.8411\n",
            "Epoch 59/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.1973 - acc: 0.9304 - val_loss: 0.6827 - val_acc: 0.8190\n",
            "Epoch 60/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.1946 - acc: 0.9304 - val_loss: 0.4544 - val_acc: 0.8612\n",
            "Epoch 61/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.1941 - acc: 0.9298 - val_loss: 0.5156 - val_acc: 0.8527\n",
            "Epoch 62/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.1915 - acc: 0.9325 - val_loss: 0.5902 - val_acc: 0.8411\n",
            "Epoch 63/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.1825 - acc: 0.9355 - val_loss: 0.4923 - val_acc: 0.8609\n",
            "Epoch 64/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.1811 - acc: 0.9353 - val_loss: 0.4458 - val_acc: 0.8728\n",
            "Epoch 65/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.1806 - acc: 0.9356 - val_loss: 0.4483 - val_acc: 0.8752\n",
            "Epoch 66/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.1762 - acc: 0.9371 - val_loss: 0.4236 - val_acc: 0.8698\n",
            "Epoch 67/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.1762 - acc: 0.9376 - val_loss: 0.4648 - val_acc: 0.8650\n",
            "Epoch 68/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.1787 - acc: 0.9371 - val_loss: 0.4835 - val_acc: 0.8618\n",
            "Epoch 69/200\n",
            "313/313 [==============================] - 52s 165ms/step - loss: 0.1700 - acc: 0.9387 - val_loss: 0.5807 - val_acc: 0.8481\n",
            "Epoch 70/200\n",
            "313/313 [==============================] - 49s 157ms/step - loss: 0.1673 - acc: 0.9412 - val_loss: 0.5449 - val_acc: 0.8497\n",
            "Epoch 71/200\n",
            "313/313 [==============================] - 49s 157ms/step - loss: 0.1675 - acc: 0.9404 - val_loss: 0.4842 - val_acc: 0.8634\n",
            "Epoch 72/200\n",
            "313/313 [==============================] - 49s 157ms/step - loss: 0.1592 - acc: 0.9438 - val_loss: 0.4648 - val_acc: 0.8669\n",
            "Epoch 73/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.1637 - acc: 0.9421 - val_loss: 0.6288 - val_acc: 0.8366\n",
            "Epoch 74/200\n",
            "313/313 [==============================] - 49s 156ms/step - loss: 0.1615 - acc: 0.9432 - val_loss: 0.5939 - val_acc: 0.8437\n",
            "Epoch 75/200\n",
            "313/313 [==============================] - 49s 155ms/step - loss: 0.1595 - acc: 0.9424 - val_loss: 0.4473 - val_acc: 0.8694\n",
            "Epoch 76/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.1563 - acc: 0.9437 - val_loss: 0.6044 - val_acc: 0.8488\n",
            "Epoch 77/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.1256 - acc: 0.9565 - val_loss: 0.3614 - val_acc: 0.8924\n",
            "Epoch 78/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.1127 - acc: 0.9602 - val_loss: 0.3926 - val_acc: 0.8873\n",
            "Epoch 79/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.1099 - acc: 0.9617 - val_loss: 0.4343 - val_acc: 0.8785\n",
            "Epoch 80/200\n",
            "313/313 [==============================] - 47s 151ms/step - loss: 0.1070 - acc: 0.9629 - val_loss: 0.3900 - val_acc: 0.8922\n",
            "Epoch 81/200\n",
            "313/313 [==============================] - 48s 152ms/step - loss: 0.1063 - acc: 0.9629 - val_loss: 0.4359 - val_acc: 0.8879\n",
            "Epoch 82/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.1032 - acc: 0.9630 - val_loss: 0.4732 - val_acc: 0.8762\n",
            "Epoch 83/200\n",
            "313/313 [==============================] - 48s 152ms/step - loss: 0.1045 - acc: 0.9626 - val_loss: 0.4433 - val_acc: 0.8805\n",
            "Epoch 84/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.1004 - acc: 0.9642 - val_loss: 0.4496 - val_acc: 0.8812\n",
            "Epoch 85/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0986 - acc: 0.9656 - val_loss: 0.4291 - val_acc: 0.8854\n",
            "Epoch 86/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0964 - acc: 0.9657 - val_loss: 0.4609 - val_acc: 0.8805\n",
            "Epoch 87/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0985 - acc: 0.9647 - val_loss: 0.5017 - val_acc: 0.8719\n",
            "Epoch 88/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.0969 - acc: 0.9664 - val_loss: 0.4518 - val_acc: 0.8827\n",
            "Epoch 89/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0947 - acc: 0.9671 - val_loss: 0.4560 - val_acc: 0.8825\n",
            "Epoch 90/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0949 - acc: 0.9664 - val_loss: 0.4800 - val_acc: 0.8792\n",
            "Epoch 91/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.0917 - acc: 0.9679 - val_loss: 0.4544 - val_acc: 0.8878\n",
            "Epoch 92/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0916 - acc: 0.9667 - val_loss: 0.4370 - val_acc: 0.8884\n",
            "Epoch 93/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0849 - acc: 0.9702 - val_loss: 0.4572 - val_acc: 0.8825\n",
            "Epoch 94/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.0912 - acc: 0.9677 - val_loss: 0.4650 - val_acc: 0.8817\n",
            "Epoch 95/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0893 - acc: 0.9693 - val_loss: 0.4635 - val_acc: 0.8883\n",
            "Epoch 96/200\n",
            "313/313 [==============================] - 48s 152ms/step - loss: 0.0849 - acc: 0.9690 - val_loss: 0.5417 - val_acc: 0.8772\n",
            "Epoch 97/200\n",
            "313/313 [==============================] - 47s 151ms/step - loss: 0.0898 - acc: 0.9676 - val_loss: 0.5498 - val_acc: 0.8706\n",
            "Epoch 98/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.0856 - acc: 0.9702 - val_loss: 0.4576 - val_acc: 0.8865\n",
            "Epoch 99/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.0835 - acc: 0.9700 - val_loss: 0.4324 - val_acc: 0.8914\n",
            "Epoch 100/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.0874 - acc: 0.9693 - val_loss: 0.4200 - val_acc: 0.8952\n",
            "Epoch 101/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0871 - acc: 0.9688 - val_loss: 0.4590 - val_acc: 0.8878\n",
            "Epoch 102/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0713 - acc: 0.9749 - val_loss: 0.4097 - val_acc: 0.8950\n",
            "Epoch 103/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.0674 - acc: 0.9764 - val_loss: 0.4469 - val_acc: 0.8895\n",
            "Epoch 104/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0654 - acc: 0.9773 - val_loss: 0.4438 - val_acc: 0.8925\n",
            "Epoch 105/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.0619 - acc: 0.9784 - val_loss: 0.4694 - val_acc: 0.8908\n",
            "Epoch 106/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0637 - acc: 0.9781 - val_loss: 0.4810 - val_acc: 0.8840\n",
            "Epoch 107/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0633 - acc: 0.9775 - val_loss: 0.4772 - val_acc: 0.8865\n",
            "Epoch 108/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.0626 - acc: 0.9778 - val_loss: 0.4963 - val_acc: 0.8817\n",
            "Epoch 109/200\n",
            "313/313 [==============================] - 48s 155ms/step - loss: 0.0641 - acc: 0.9779 - val_loss: 0.4412 - val_acc: 0.8955\n",
            "Epoch 110/200\n",
            "313/313 [==============================] - 48s 152ms/step - loss: 0.0644 - acc: 0.9770 - val_loss: 0.4782 - val_acc: 0.8867\n",
            "Epoch 111/200\n",
            "313/313 [==============================] - 48s 152ms/step - loss: 0.0653 - acc: 0.9772 - val_loss: 0.4668 - val_acc: 0.8902\n",
            "Epoch 112/200\n",
            "313/313 [==============================] - 48s 152ms/step - loss: 0.0611 - acc: 0.9786 - val_loss: 0.4597 - val_acc: 0.8907\n",
            "Epoch 113/200\n",
            "313/313 [==============================] - 48s 153ms/step - loss: 0.0584 - acc: 0.9802 - val_loss: 0.4661 - val_acc: 0.8916\n",
            "Epoch 114/200\n",
            "313/313 [==============================] - 48s 152ms/step - loss: 0.0603 - acc: 0.9789 - val_loss: 0.5005 - val_acc: 0.8883\n",
            "Epoch 115/200\n",
            "313/313 [==============================] - 48s 152ms/step - loss: 0.0555 - acc: 0.9801 - val_loss: 0.4533 - val_acc: 0.8946\n",
            "Epoch 116/200\n",
            "313/313 [==============================] - 48s 152ms/step - loss: 0.0567 - acc: 0.9799 - val_loss: 0.5179 - val_acc: 0.8815\n",
            "Epoch 117/200\n",
            "313/313 [==============================] - 48s 154ms/step - loss: 0.0568 - acc: 0.9809 - val_loss: 0.4709 - val_acc: 0.8888\n",
            "Epoch 118/200\n",
            "313/313 [==============================] - 53s 168ms/step - loss: 0.0550 - acc: 0.9805 - val_loss: 0.4335 - val_acc: 0.8979\n",
            "Epoch 119/200\n",
            "117/313 [==========>...................] - ETA: 28s - loss: 0.0546 - acc: 0.9796"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVwy8sHG9nol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nb7uwQ3YqeB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyqLQzAZRCHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('NguyenVanPhat.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zeu1UU-ixe8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "cd114207-904f-407c-cfd7-3df4896ebf54"
      },
      "source": [
        "# Code load model to test\n",
        "from keras.models import load_model\n",
        "model = load_model('NguyenVanPhat.h5')\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "10000/10000 [==============================] - 8s 838us/step\n",
            "Test loss: 0.5453910734206439\n",
            "Test accuracy: 0.8906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAo35ACEqBUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}